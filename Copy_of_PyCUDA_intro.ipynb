{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PyCUDA_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomKellyGenetics/toy_snp_caller/blob/master/Copy_of_PyCUDA_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31d0lBm-Lm6o",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "First of all you need to check whether you have a GPU-accelerated Runtime. To do this, Select the 'Runtime' menu and then choose 'Change runtime type'. You should then choose 'GPU' if it is not already selected. Once done, you may need to select the 'Connect' option at the top right of your notebook window. \n",
        "When it has loaded and connected you should run the following command to check whether the system is indeed using a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjnf4EoLLQqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "07796cd7-037b-4755-ef83-03c5b190b8d1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 24 02:14:38 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24l8u6jxM0rH",
        "colab_type": "text"
      },
      "source": [
        "If all is well, you should see some output that includes the type of GPU being used (k80) and lists any processed running (none for the moment).\n",
        "\n",
        "Next, we need to install PyCUDA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOSSKBYcPK2y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0VbZq97ONuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "70cb170d-1079-4470-8fbd-f99f2814c45f"
      },
      "source": [
        "pip install pycuda"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/3f/5658c38579b41866ba21ee1b5020b8225cec86fe717e4b1c5c972de0a33c/pycuda-2019.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.5MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/96/00416762a3eda8876a17d007df4a946f46b2e4ee1057e0b9714926472ef8/pytools-2019.1.1.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools, mako\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2019.1.2-cp36-cp36m-linux_x86_64.whl size=4535904 sha256=87b234838308f641ed6e1ac9bb255194141f4fd08268c882c1e827a0eb4ee421\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/60/f0/b1c430c73d281ac3e46070480db50f7907364eb6f6d3188396\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2019.1.1-py2.py3-none-any.whl size=58424 sha256=c8a1a926035866f7cb73c4099472bba4ad805ce4f1ccce44fcdc275b609ef9cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/df/0b/75ac4572aaa93e3eba6a58472635d0fda907f5f4cf884a3a0c\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mako: filename=Mako-1.1.0-cp36-none-any.whl size=75363 sha256=43435dd20a0cd1053a03048a2f517ec79bb7ea15da51a3080ef113ebb9460dc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "Successfully built pycuda pytools mako\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.1.0 pycuda-2019.1.2 pytools-2019.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r3S8FLxPNz0",
        "colab_type": "text"
      },
      "source": [
        "Now we should be able to import the PyCUDA libraries that we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUxj_OKmNQg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l11ln93PYwX",
        "colab_type": "text"
      },
      "source": [
        "We will be using numpy quite a lot so let's import that right away:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNoxx_f-PzhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVY_apz0QTb5",
        "colab_type": "text"
      },
      "source": [
        "Okay, let's create a 4 x 4 array using numpy's random function and show the contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6NdnDdoP8K5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "575c1713-c5d3-4606-b900-f6d377d32904"
      },
      "source": [
        "a = np.random.randn(4,4)\n",
        "a"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.24375605, -2.05606006, -1.05551482, -1.87345212],\n",
              "       [-0.50008479,  0.81762928, -0.79785209,  0.5618535 ],\n",
              "       [ 1.20853345, -0.38096592, -0.94690202,  0.06241843],\n",
              "       [-0.71626338, -0.2208585 , -1.57586999,  0.05438271]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTfLWQ7Q3Ns",
        "colab_type": "text"
      },
      "source": [
        "Let's check the type of one of the elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsXrbyKiQnhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df924839-898a-4b1a-d6a8-f7fd4e97b4c5"
      },
      "source": [
        "type(a[0,0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUESvwNERCT-",
        "colab_type": "text"
      },
      "source": [
        "As you should see, this is of type float64. This is not always the right data type for kernel functions for the following reasons:\n",
        "Only Enterprise GPUs (i.e. Volta/Pascal rather than GeForce/RTX) tend to support Full Precision (float64). Even then, they tend to have better support for lower precision (e.g. int32, fp16 etc.)\n",
        "So it's better to either specify the type or cast it before passing it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTys3x--SiBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a107d466-c359-47b5-ca11-cc4cf9779dee"
      },
      "source": [
        "b = np.random.randn(4,4)\n",
        "b = b.astype(np.float32)\n",
        "print(b)\n",
        "print(type(b[0,0]))\n",
        "print(\"\")\n",
        "c = np.random.randint(low=1, high=100, size=16, dtype=np.int16).reshape(4,4)\n",
        "print(c)\n",
        "print(type(c[0,0]))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.37104166 -0.13156942  0.9888739   0.43702406]\n",
            " [ 0.17276785 -0.66560954  1.226529   -0.3124692 ]\n",
            " [ 0.5235419   3.1243346   0.66348433 -0.8306269 ]\n",
            " [-0.909838   -1.725645    0.34903932 -0.23539452]]\n",
            "<class 'numpy.float32'>\n",
            "\n",
            "[[17 60 54  8]\n",
            " [52 68 50 90]\n",
            " [26 10 56 63]\n",
            " [25 31 28 16]]\n",
            "<class 'numpy.int16'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh6tEEn7Xoxx",
        "colab_type": "text"
      },
      "source": [
        "Okay, now let's take a look at the way our data can be transferred over to the device (GPU) for a kernel to act upon.\n",
        "In this example, we use the cuda.mem_alloc to allocate some gpu memory. In order to do this, we need to determine how much memory we need. For this we use the nbytes property of our variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grrpCarZSTIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e13a831d-0932-4691-df63-d1d2ddf7ee90"
      },
      "source": [
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "print(str(b.nbytes) + \" bytes allocated\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 bytes allocated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftKgGFYqZNRQ",
        "colab_type": "text"
      },
      "source": [
        "That has allocated the memory, so now we can transfer the data to the gpu memory (htod = host to device):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilnj0yVIZXjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1bfd6a1-36e4-4f89-8679-3264c036eb1c"
      },
      "source": [
        "cuda.memcpy_htod(b_gpu, b)\n",
        "print(str(b.nbytes) + \" allocated to GPU: \" + str(b_gpu.__sizeof__()))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 allocated to GPU: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfkhGj32aCal",
        "colab_type": "text"
      },
      "source": [
        "Next we need to create some code to execute on the GPU - our kernel function - that will operate on the data we just passed.\n",
        "\n",
        "In this case all we are going to do is multiply the contents of the a matrix by 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLUQDP2abuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a801763f-5861-43c6-c510-d94e1a78b541"
      },
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    // define index (flatten 4x4 matrix) to 1-dimension array (1x16 vector)\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "  \"\"\")\n",
        "print(\"kernel successfully initiated\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kernel successfully initiated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3lU5sNa5mb",
        "colab_type": "text"
      },
      "source": [
        "If there were no errors then our kernel has compiled and is ready to be called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmaDMGh7bDKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assign the C function \"doublify\" as a python function \"func\" and run as func on GPU\n",
        "func = mod.get_function(\"doublify\")\n",
        "func(b_gpu, block=(4,4,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_sbUERnbZjC",
        "colab_type": "text"
      },
      "source": [
        "Let's just take a look at what we just executed.\n",
        "First we got a handle to the doublify function that we just created, then we called the function passing a_gpu as the function argument and we specified the launch configuration using the block parameter - more on this shortly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYwqtV2lcIj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8cb48767-c4f4-4de9-9e6a-9d17e8c2d8c6"
      },
      "source": [
        "b_doubled = np.empty_like(b)\n",
        "cuda.memcpy_dtoh(b_doubled, b_gpu)\n",
        "print(b_doubled)\n",
        "print(b)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.7420833  -0.26313883  1.9777478   0.8740481 ]\n",
            " [ 0.3455357  -1.3312191   2.453058   -0.6249384 ]\n",
            " [ 1.0470839   6.248669    1.3269687  -1.6612538 ]\n",
            " [-1.819676   -3.45129     0.69807863 -0.47078905]]\n",
            "[[ 0.37104166 -0.13156942  0.9888739   0.43702406]\n",
            " [ 0.17276785 -0.66560954  1.226529   -0.3124692 ]\n",
            " [ 0.5235419   3.1243346   0.66348433 -0.8306269 ]\n",
            " [-0.909838   -1.725645    0.34903932 -0.23539452]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrV98pzpEvs",
        "colab_type": "text"
      },
      "source": [
        "Here you can see that every element of the matrix has been doubled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuzJ5L3fnlRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3a7e35b5-478e-4e03-e8d5-735b88a96727"
      },
      "source": [
        "b_doubled / b"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 2., 2., 2.],\n",
              "       [2., 2., 2., 2.],\n",
              "       [2., 2., 2., 2.],\n",
              "       [2., 2., 2., 2.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgh7cxqgnrcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1417f464-a77d-4f98-b94f-b2269af4d47c"
      },
      "source": [
        "print(b[1,1]) \n",
        "print(b_doubled[1,1])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.66560954\n",
            "-1.3312191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcEfBK2e88m",
        "colab_type": "text"
      },
      "source": [
        "Okay, so that worked (hopefully) but what did the kernel code actually do? \n",
        "To more fully understand this we need to take a look at the way that kernels are launched (Slides will be presented).\n",
        "\n",
        "**Part 2**\n",
        "\n",
        "To better understand what is going on when you launch a kernel let's try a few experiments.\n",
        "\n",
        "We'll create a simple kernel that just sets the elements of the input array to the thread index. This should help show what is going on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsC5AkKgBuWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod2 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x;\n",
        "    a[idx] = threadIdx.x;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-drv0TiZuMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "513b573a-6b9c-4c25-8497-7eed27b87c6c"
      },
      "source": [
        "# Create an empty 1d array\n",
        "d = np.zeros((16), dtype=np.uint32)\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func2 = mod2.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 1 block\n",
        "func2(d_gpu, block=(4,1,1))\n",
        "\n",
        "# Create array for output\n",
        "d_out = np.empty_like(d)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Print data out\n",
        "print(d_out)\n",
        "\n",
        "# Free up the memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaIq0qV6cR2C",
        "colab_type": "text"
      },
      "source": [
        "Do you understand what the output has produced? \n",
        "Our kernel has launched a block with 4 threads (block = (4,1,1) means that 4 threads are created in the block on the x dimension, with no additional threads on the y or z dimensions). Each thread has written its threadIdx.x value into the array element corresponding to this value. Since there are only 4 threads, only the first 4 elements have been updated.\n",
        "If we want to write into the other elements we will have to do one of several things:\n",
        "\n",
        "*   Increase the number of threads in the block\n",
        "*   Increase the number of blocks\n",
        "*   Iterate within the kernel\n",
        "\n",
        "Let's try these one by one.\n",
        "\n",
        "By changing the number of threads per block to 16 (in the x dimension) we can fully populate the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sGcy_3dzMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f274d17-01eb-44a9-dbc0-7218d24bfc24"
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Create a lauch configuration with 16 threads\n",
        "func2(d_gpu, block=(16,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN7jDBz7Nkt4",
        "colab_type": "text"
      },
      "source": [
        "In order to make increasing the number of blocks effective we will need to change the kernel so that it can use them. Without doing this each block will simply write into the same array element as the others. By adding the blockIdx.x into the kernel we can use it to map each thread and block to a specific element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07MPK31Omp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod3 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    // define index so that each block processes a separate thread\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    a[idx] = idx;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNR9XHJyMwuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc60b1f7-fade-4cfa-c290-cdcda31c8c0f"
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ0VkVlzq7Dh",
        "colab_type": "text"
      },
      "source": [
        "We can see that these components are important to assign a thread index to each block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuUX1X0wq6Wv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d525abe-72cf-4f43-a9be-1be1169fa9a8"
      },
      "source": [
        "mod3 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    // define index so that each block processes a separate thread\n",
        "    int idx = threadIdx.x * blockDim.x;\n",
        "    a[idx] = idx;\n",
        "  }\n",
        "  \"\"\")\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  0  0  0  8  0  0  0 12  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXcj8-pSrEXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a28795d-b15b-4d6c-ed59-51b2a223a968"
      },
      "source": [
        "mod3 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    # define index so that each block processes a separate thread\n",
        "    int idx = threadIdx.x * blockIdx.x;\n",
        "    a[idx] = idx;\n",
        "  }\n",
        "  \"\"\")\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 0 6 0 0 9 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3XhQVqrrfCb",
        "colab_type": "text"
      },
      "source": [
        "We miust also ensure that the block/grid configuration has enough spaces for all threads in the index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsorLhw9rV-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ec065a8-c1a4-4f46-b1ee-de8612d4123e"
      },
      "source": [
        "mod3 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    // define index so that each block processes a separate thread\n",
        "    int idx = threadIdx.x * blockIdx.x;\n",
        "    a[idx] = idx;\n",
        "  }\n",
        "  \"\"\")\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(5,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  0  6  0  8  9  0  0 12  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Got-CQRVvm",
        "colab_type": "text"
      },
      "source": [
        "Lets try some iteration within the kernel next. This is only really used when the size of the problem exceeds the total number of threads available on the gpu but, for the sake of illustration we'll try it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOFDZY1Itbg1",
        "colab_type": "text"
      },
      "source": [
        "This runs indexes in a loop on a smaller block than the number of indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtJ7QYbNR5fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod4 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x;\n",
        "    for (int i=0;i<16;i+=4)\n",
        "    {\n",
        "      a[idx + i] = idx + i;\n",
        "    }\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dpd7M-1Tesh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db9435c4-4584-4f4b-cc86-7021f6d4586b"
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod4.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3qpuTdnUP42",
        "colab_type": "text"
      },
      "source": [
        "Okay, so as a final exercise, let's see if the following makes sense. Note that we are using a 3D matrix as input this time. However, as far as the C-based kernel is concerned, there is no difference between a 3D array and a 1D array - ultimately they are both just contiguous memory.\n",
        "Check that you understand the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQ4aw8gJZu7W",
        "colab": {}
      },
      "source": [
        "mod5 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    a[idx] = threadIdx.x;\n",
        "    a[idx + 1] = blockIdx.x;\n",
        "    a[idx + 2] = blockDim.x;\n",
        "    a[idx + 3] = gridDim.x;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwr8Ok8WGheO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "0aa63139-66cd-490c-ca56-bd5c63db5b11"
      },
      "source": [
        "# Create an empty 3d array\n",
        "b = np.zeros((4,4,4), dtype=np.uint32)\n",
        "# Allocate GPU memory\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "\n",
        "# Get the function handle\n",
        "func = mod5.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func(b_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Create array for output\n",
        "b_out = np.empty_like(b)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(b_out, b_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(b_out.reshape((4,4,4)))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 4 4]\n",
            "  [1 0 4 4]\n",
            "  [2 0 4 4]\n",
            "  [3 0 4 4]]\n",
            "\n",
            " [[0 1 4 4]\n",
            "  [1 1 4 4]\n",
            "  [2 1 4 4]\n",
            "  [3 1 4 4]]\n",
            "\n",
            " [[0 2 4 4]\n",
            "  [1 2 4 4]\n",
            "  [2 2 4 4]\n",
            "  [3 2 4 4]]\n",
            "\n",
            " [[0 3 4 4]\n",
            "  [1 3 4 4]\n",
            "  [2 3 4 4]\n",
            "  [3 3 4 4]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qKl-fvuU2P1",
        "colab_type": "text"
      },
      "source": [
        "Try changing some of the parameters such as block size and see what happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr1jqaQ-t583",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b22c7dcd-209f-4c85-ff5e-34aeb4743de8"
      },
      "source": [
        "# Create an empty 3d array\n",
        "b = np.zeros((6,3,4), dtype=np.uint32)\n",
        "# Allocate GPU memory\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "\n",
        "# Get the function handle\n",
        "func = mod5.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func(b_gpu, block=(2,2,1), grid=(6,3,1))\n",
        "\n",
        "# Create array for output\n",
        "b_out = np.empty_like(b)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(b_out, b_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(b_out.reshape((6,3,4)))\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 2 6]\n",
            "  [1 0 2 6]\n",
            "  [0 1 2 6]]\n",
            "\n",
            " [[1 1 2 6]\n",
            "  [0 2 2 6]\n",
            "  [1 2 2 6]]\n",
            "\n",
            " [[0 3 2 6]\n",
            "  [1 3 2 6]\n",
            "  [0 4 2 6]]\n",
            "\n",
            " [[1 4 2 6]\n",
            "  [0 5 2 6]\n",
            "  [1 5 2 6]]\n",
            "\n",
            " [[0 3 4 4]\n",
            "  [1 3 4 4]\n",
            "  [2 3 4 4]]\n",
            "\n",
            " [[3 3 4 4]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBEwFCxEuBGq",
        "colab_type": "text"
      },
      "source": [
        "Finally, see if you can use what you have learned to create kernel that multiplies the elements in matrix a by matrix b. The outline has been provided. Please see if you can complete the code and produce a correct result!\n",
        "The assertion will fail if you don't. Remember to edit the launch configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4xFLgyvAN2e",
        "colab_type": "text"
      },
      "source": [
        "Compare to serial version \n",
        "\n",
        " - in python?\n",
        " \n",
        " - with a 1x1x1 grid?\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uITt2WiVhEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mmult = SourceModule(\"\"\"\n",
        "  __global__ void multiply(int *a, int *b)\n",
        "  {\n",
        "    // initialise indices\n",
        "    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    \n",
        "    \n",
        "    int a_idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    int b_idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    // initialise output matrix\n",
        "    int ab = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    // multiply each element of a by b \n",
        "    \n",
        "    \n",
        "    //multiply\n",
        "    \n",
        "    // vector for each element of AB\n",
        "    \n",
        "    //sum each element\n",
        "    ab[idx] = a[a_idx] * a[a_idx];\n",
        "    \n",
        "    \n",
        "    int idx = threadIdx.x;\n",
        "    for (int i=0;i<16;i+=4)\n",
        "    {\n",
        "      a[idx + i] = idx + i;\n",
        "    }\n",
        "    \n",
        "   \n",
        "    // TODO\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "a = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]],dtype=np.uint32)\n",
        "b = np.array([[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]],dtype=np.uint32)\n",
        "\n",
        "# Allocate GPU memory\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "# set the gpu memory\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "\n",
        "# Get the function handle\n",
        "func = mmult.get_function(\"multiply\")\n",
        "# Create a lauch configuration with 4 threads and 1 block\n",
        "func(a_gpu, b_gpu, TODO ......)\n",
        "\n",
        "# Create array for output\n",
        "a_out = np.empty_like(a)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(a_out, a_gpu)\n",
        "\n",
        "# Print data out\n",
        "print(a_out)\n",
        "\n",
        "# Free up the memory\n",
        "a_gpu.free()\n",
        "b_gpu.free()\n",
        "\n",
        "assert (a_out == [[1,2,3,4],[2,4,6,8],[3,6,9,12],[4,8,12,16]]).all()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMIyO-97y4QP",
        "colab_type": "text"
      },
      "source": [
        "There is a simpler way of handling arrays than what we have so far looked at. The GPUArray is an abstraction that removes some of the steps. It is useful to understand what those steps did though, because it helps your understanding of what is going on, which is helpful when it comes to getting the best performance from your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCuy7gFzg9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pycuda import gpuarray\n",
        "\n",
        "x = np.random.randn(5).astype(np.float32)\n",
        "x_gpu = gpuarray.to_gpu(x)\n",
        "\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87omznK02qw",
        "colab_type": "text"
      },
      "source": [
        "You can operate directly on gpuarrays as if they were normal numpy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzJHO54Z3GAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_gpu = x_gpu * 3\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1PZq0gp3gNY",
        "colab_type": "text"
      },
      "source": [
        "GPUArrays also support slicing. The one thing that you cannot do is indexing though:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elYXoJ6Y3wrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_gpu[:3]\n",
        "\n",
        "x_gpu[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhdgyxHX4Z_k",
        "colab_type": "text"
      },
      "source": [
        "In order to use a GPUArray in a function created with SourceModule (as per the examples above) we need to use the **gpudata** attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwGPGvl5JXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " func = mod.get_function(\"doublify\")\n",
        " func(x_gpu.gpudata, block = (5,1,1))\n",
        "\n",
        " x_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqpyFFM6YOk",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this is much cleaner than the previous method! \n",
        "\n",
        "Next we need to explain a couple of important concepts that, so far, we ignored.\n",
        "First of all, you may have noticed that the kernel function we declared was preceeded by the \\_\\_global\\_\\_ keyword. This tells the compiler that this is kernel (i.e. it will be called from the CPU but executed on the GPU). You will also have seen that the return type was 'void'. This is because kernel function cannot return values. Instead, any results need to be handled by passing in a pointer to a variable and manipulating the data within the kernel.\n",
        "\n",
        "Additionally, sometimes you need to call a function from within your kernel. This can be done by declaring a \\_\\_device\\_\\_ function, which will reside only on the GPU.\n",
        "\n",
        "Let's create a simple example of a device function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKzcBhRgCa2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = SourceModule(\"\"\"\n",
        "__device__ int times2( float px); // declaration\n",
        "  __global__ void  matAdd(float *a)  \n",
        "{  \n",
        "        int idx = blockIdx.x*blockDim.x+threadIdx.x;  \n",
        "        \n",
        "        a[idx] = times2(idx);   \n",
        "\n",
        "}  \n",
        "  __device__ int times2( float idx) // implementation\n",
        "{\n",
        "    return idx * 2; \n",
        "}  \n",
        "\"\"\")\n",
        "\n",
        "func = dev.get_function(\"matAdd\")\n",
        "func(x_gpu.gpudata, block = (5,1,1))\n",
        "\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZKjydn8uwPG",
        "colab_type": "text"
      },
      "source": [
        "Okay, next see if you can create your own matrix multiplication kernel. Remember that there is a reduction operation in a matrix multiplication that needs to be run after the multiplication, so watch out for data races!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_PQ4U1hvPPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your kernel here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "204LHvxpvV0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your calling code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1a_Eyivbpf",
        "colab_type": "text"
      },
      "source": [
        "Well done (or cmomiserations)!\n",
        "\n",
        "You have completed the introduction."
      ]
    }
  ]
}